{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import treeUtil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from readability import Readability\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "[lib, con, neutral] = pickle.load(open('ibcData.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the leaf nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leaf = lib[0].get_leaves()\n",
    "# print(len(leaf))\n",
    "# leaf[3].print_leaf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how to access sentence text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print ('Liberal examples (out of ', len(lib), ' sentences): ')\n",
    "# for tree in lib[0:5]:\n",
    "#     print (tree.get_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print ('\\nConservative examples (out of ', len(con), ' sentences): ')\n",
    "# for tree in con[0:5]:\n",
    "#     print (tree.get_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print ('\\nNeutral examples (out of ', len(neutral), ' sentences): ')\n",
    "# for tree in neutral[0:5]:\n",
    "#     print (tree.get_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listFromTree(tree):\n",
    "    lst = []\n",
    "    for ex_tree in tree:\n",
    "        lst.append(ex_tree.get_words())\n",
    "#        for node in ex_tree:\n",
    "\n",
    "#         # remember, only certain nodes have labels (see paper for details)\n",
    "#         if hasattr(node, 'label'):\n",
    "#             if node.label == \"Liberal\":\n",
    "#                 liberal.append(node.get_words())\n",
    "#             elif node.label == \"Neutral\":\n",
    "#                 neutral.append(node.get_words())\n",
    "#             elif node.label == \"Conservative\":\n",
    "#                 conserv.append(node.get_words())\n",
    "#                 (node.label, ': ', node.get_words())\n",
    "#     #             print (type(node.get_words()))\n",
    "    return lst\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listsFromTrees(treelib, treeneu, treecon):\n",
    "    lstLib = []\n",
    "    lstNeu = []\n",
    "    lstCon = []\n",
    "    wordLen = 6\n",
    "    for ex_tree in treelib:\n",
    "        #lst.append(ex_tree.get_words())\n",
    "        for node in ex_tree:\n",
    "        # remember, only certain nodes have labels (see paper for details)\n",
    "            if hasattr(node, 'label') and len(node.get_words().split()) > wordLen:\n",
    "                if node.label == \"Liberal\":\n",
    "                    lstLib.append(node.get_words())\n",
    "                elif node.label == \"Neutral\":\n",
    "                    lstNeu.append(node.get_words())\n",
    "                elif node.label == \"Conservative\":\n",
    "                    lstCon.append(node.get_words())\n",
    "\n",
    "    for ex_tree in treeneu:\n",
    "        #lst.append(ex_tree.get_words())\n",
    "        for node in ex_tree:\n",
    "        # remember, only certain nodes have labels (see paper for details)\n",
    "            if hasattr(node, 'label') and len(node.get_words().split()) > wordLen:\n",
    "                if node.label == \"Liberal\":\n",
    "                    lstLib.append(node.get_words())\n",
    "                elif node.label == \"Neutral\":\n",
    "                    lstNeu.append(node.get_words())\n",
    "                elif node.label == \"Conservative\":\n",
    "                    lstCon.append(node.get_words())\n",
    "                \n",
    "    for ex_tree in treecon:\n",
    "        #lst.append(ex_tree.get_words())\n",
    "        for node in ex_tree:\n",
    "        # remember, only certain nodes have labels (see paper for details)\n",
    "            if hasattr(node, 'label') and len(node.get_words().split()) > wordLen:\n",
    "                if node.label == \"Liberal\":\n",
    "                    lstLib.append(node.get_words())\n",
    "                elif node.label == \"Neutral\":\n",
    "                    lstNeu.append(node.get_words())\n",
    "                elif node.label == \"Conservative\":\n",
    "                    lstCon.append(node.get_words())\n",
    "    return lstLib, lstNeu, lstCon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025\n",
      "600\n",
      "1701\n"
     ]
    }
   ],
   "source": [
    "# how to access phrase labels for a particular tree\n",
    "#ex_tree = lib[1]\n",
    "\n",
    "liberal = []\n",
    "neut = []\n",
    "conserv = []\n",
    "\n",
    "#Only sentences\n",
    "liberal = listFromTree(lib)\n",
    "neut = listFromTree(neutral)\n",
    "conserv = listFromTree(con)\n",
    "\n",
    "#Sentences and phrases\n",
    "#liberal, neut, conserv = listsFromTrees(lib, neutral, con)\n",
    "\n",
    "print (len(liberal))\n",
    "print (len(neut))\n",
    "print (len(conserv))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanList(dataList):\n",
    "    newList = []\n",
    "    #lowercase\n",
    "    newList = [x.lower() for x in dataList]\n",
    "    #removes punctuations\n",
    "    newList = [s.translate(str.maketrans('', '', string.punctuation)) for s in  newList]\n",
    "    #removes stop words\n",
    "    stop = stopwords.words('english')\n",
    "    for i in range(0, len(newList)):\n",
    "        newList[i] = \" \".join(x for x in newList[i].split() if x not in stop)\n",
    "    return newList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025\n",
      "600\n",
      "1701\n"
     ]
    }
   ],
   "source": [
    "liberal = cleanList(liberal)\n",
    "neut = cleanList(neut)\n",
    "conserv = cleanList(conserv)\n",
    "print (len(liberal))\n",
    "print (len(neut))\n",
    "print (len(conserv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = pd.DataFrame({\"liberal\":liberal})\n",
    "n = pd.DataFrame({\"neutral\":neut})\n",
    "c = pd.DataFrame({\"conservative\":conserv})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([l,n,c], axis=1)\n",
    "data['liberal'] = data['liberal'].fillna('')\n",
    "data['neutral'] = data['neutral'].fillna('')\n",
    "data['conservative'] = data['conservative'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topTen(dataFr, index):\n",
    "    frequency = pd.Series(' '.join(dataFr[index]).split()).value_counts()[:10]\n",
    "    frequency = list(frequency.index)\n",
    "    return dataFr[index].apply(lambda x: \" \".join(x for x in x.split() if x not in frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bottomTen(dataFr, index):\n",
    "    frequency = pd.Series(' '.join(dataFr[index]).split()).value_counts()[-10:]\n",
    "    frequency = list(frequency.index)\n",
    "    return dataFr[index].apply(lambda x: \" \".join(x for x in x.split() if x not in frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removes top 10 words\n",
    "data['neutral'] = topTen(data, \"neutral\")\n",
    "data['liberal'] = topTen(data, \"liberal\")\n",
    "data['conservative'] = topTen(data, \"conservative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removes top 10 rare words\n",
    "data['neutral'] = bottomTen(data, \"neutral\")\n",
    "data['liberal'] = bottomTen(data, \"liberal\")\n",
    "data['conservative'] = bottomTen(data, \"conservative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmat(dataFr, index):\n",
    "    newData =dataFr\n",
    "    for i in range(0, len(dataFr[index])):\n",
    "        newData[index][i] = \" \".join([Word(word).lemmatize() for word in dataFr[index][i].split()])\n",
    "    return newData[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "data['neutral'] = lemmat(data, \"neutral\")\n",
    "data['liberal'] = lemmat(data, \"liberal\")\n",
    "data['conservative'] = lemmat(data, \"conservative\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88949"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for col in data.columns:\n",
    "all_text = \" \".join(x for col in data.columns for x in data[col])\n",
    "words = all_text.split()\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "polit_ints = []\n",
    "for col in data.columns:\n",
    "    for each in data[col]:\n",
    "        if each != \"\":\n",
    "            polit_ints.append([vocab_to_int[word] for word in each.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4326\n"
     ]
    }
   ],
   "source": [
    "print(len(polit_ints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "# x_train = x_train + [x for x in data[\"liberal\"]] \n",
    "# x_train = x_train + [x for x in data[\"neutral\"][0:600]]\n",
    "# x_train = x_train + [x for x in data[\"conservative\"][0:1701]]\n",
    "\n",
    "y_train = y_train + ([1] * len(liberal))\n",
    "y_train = y_train + ([0] * len(neut))\n",
    "y_train = y_train + ([-1] * len(conserv))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length sentences: 0\n",
      "Maximum sentence length: 50\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "polit_lens = Counter([len(x) for x in polit_ints])\n",
    "print(\"Zero-length sentences: {}\".format(polit_lens[0]))\n",
    "print(\"Maximum sentence length: {}\".format(max(polit_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0, 1389,  438,   20,  581,  377,  137,   54, 3292,   43,  539,\n",
       "         110, 7108, 1039,  300, 1143,  196],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0, 3293,  802,  144, 5015,\n",
       "        2802, 1288, 1040,  413, 2164, 5016,  841,  131, 1390,  100,  248,\n",
       "         157, 1289, 3914, 1290, 3294,  561],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,  582, 5017,  329,  193,  104,  193,  145,\n",
       "        3915,   20, 7109,  583,   73,   80],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0, 2446,  842, 5018,  528,\n",
       "        1763,  745,   44,   29, 2803, 7110, 7111,  138,   11,   27, 5019,\n",
       "        5020, 7112, 3916, 1391,    1,  414],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0, 2165, 1499,    7,   40, 3295, 2166,  189,  770,  472,   75,\n",
       "         225,   80,  263, 2804,  895, 5021],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0, 5022,  104, 2805,  609,  415,  239,   54,\n",
       "         938, 1622, 1080, 7113, 3917,  259],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "         351, 3296,  641,   41, 7114, 5023,  352,   43,   65,  584,  154,\n",
       "         121, 7115,  439, 7116,  803,   43],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    3,  147,  540,  225,\n",
       "        7117,  804,  116,   61,  642,  173],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,   65,  148, 1392, 7118, 7119,  939,\n",
       "          34,   47, 1623,  452,  643, 3297,  330,   97, 5024,    1,   38,\n",
       "          16,  212,   88,   14,  378,  138],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,  843,   45,  805,   28,  488, 7120,  277, 2806,  277,\n",
       "          33, 2807,   54, 5025,  416, 3298,    4,   54,  507,   40,  256,\n",
       "          28, 2167,  896, 1393,   81,  353]])"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 50\n",
    "features = np.zeros((len(polit_ints), seq_len), dtype=int)\n",
    "# print(features[:10,:100])\n",
    "for i, row in enumerate(polit_ints):\n",
    "    features[i, -len(row):] = np.array(row)[:seq_len]\n",
    "features[:10,:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4326\n",
      "4326\n",
      "<class 'numpy.ndarray'>\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0   17  616  940  849  491 1210  510  993  204  940 2822  123  438\n",
      "    1 5059 5060   22 3921  193 3931 3932]\n",
      "50\n",
      "[17, 616, 940, 849, 491, 1210, 510, 993, 204, 940, 2822, 123, 438, 1, 5059, 5060, 22, 3921, 193, 3931, 3932]\n",
      "21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(features))\n",
    "print(len(labels))\n",
    "print(type(features))\n",
    "print(features[41])\n",
    "print(len(features[41]))\n",
    "print(polit_ints[41])\n",
    "print(len(polit_ints[41]))\n",
    "print()\n",
    "for item in features: \n",
    "    if item[49] == 0:\n",
    "        print( item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,   300,  1143,   196],\n",
       "       [    0,     0,     0, ...,  1290,  3294,   561],\n",
       "       [    0,     0,     0, ...,   583,    73,    80],\n",
       "       ...,\n",
       "       [    0,     0,     0, ..., 14008,  2603,  2430],\n",
       "       [    0,     0,     0, ...,   188,   259,  1883],\n",
       "       [    0,     0,     0, ...,  2818,  7058,  1550]])"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405\n",
      "120\n",
      "341\n",
      "4326\n",
      "\n",
      "202\n",
      "465\n",
      "695\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "\n",
    "len_lib = len(liberal)\n",
    "len_neu = len(neut)\n",
    "len_con =  len(conserv)\n",
    "\n",
    "\n",
    "split_index_lib = int(split_frac * len(liberal))\n",
    "split_index_neu = int(len_lib + int(split_frac * len(neut)))\n",
    "split_index_con =int(len_lib + len_neu + int(split_frac * len(conserv)))\n",
    "\n",
    "\n",
    "\n",
    "split_frac = 0.5\n",
    "\n",
    "size_lib = len_lib-split_index_lib\n",
    "size_neu = len_lib+len_neu-split_index_neu\n",
    "size_con = len(features)-split_index_con\n",
    "\n",
    "\n",
    "split_index_lib = int(split_frac * size_lib)\n",
    "split_index_neu = int(size_lib + int(split_frac * size_neu))\n",
    "split_index_con = int(size_lib + size_neu + int(split_frac * size_con))\n",
    "\n",
    "print(size_lib)\n",
    "print(size_neu)\n",
    "print(size_con)\n",
    "print(len(features))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "print(split_index_lib)\n",
    "print(split_index_neu)\n",
    "print(split_index_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1701"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(conserv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(3460, 50) \n",
      "Validation set: \t(433, 50) \n",
      "Test set: \t\t(433, 50)\n",
      "label set: \t\t(3460,) \n",
      "Validation label set: \t(433,) \n",
      "Test label set: \t\t(433,)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "\n",
    "len_lib = len(liberal)\n",
    "len_neu = len(neut)\n",
    "len_con =  len(conserv)\n",
    "\n",
    "\n",
    "split_index_lib = int(split_frac * len(liberal))\n",
    "split_index_neu = int(len_lib + int(split_frac * len(neut)))\n",
    "split_index_con =int(len_lib + len_neu + int(split_frac * len(conserv)))\n",
    "\n",
    "\n",
    "# 2025\n",
    "# 600\n",
    "# 1701\n",
    "\n",
    "# 7809\n",
    "# 8843\n",
    "# 5969\n",
    "\n",
    "\n",
    "train_x = np.array(features[:split_index_lib].tolist() + features[len_lib:split_index_neu].tolist() + features[len_lib+len_neu:split_index_con].tolist())\n",
    "var_x = np.array(features[split_index_lib:len_lib].tolist() + features[split_index_neu:len_lib+len_neu].tolist() +   features[split_index_con:].tolist()) \n",
    "train_y = np.array(labels[:split_index_lib].tolist() + labels[len_lib:split_index_neu].tolist() + labels[len_lib+len_neu:split_index_con].tolist())\n",
    "var_y = np.array(labels[split_index_lib:len_lib].tolist() + labels[split_index_neu:len_lib+len_neu].tolist() +   labels[split_index_con:].tolist()) \n",
    "\n",
    "\n",
    "split_frac = 0.5\n",
    "\n",
    "size_lib = len_lib-split_index_lib\n",
    "size_neu = len_lib+len_neu-split_index_neu\n",
    "size_con = len(features)-split_index_con\n",
    "\n",
    "\n",
    "split_index_lib = int(split_frac * size_lib)\n",
    "split_index_neu = int(size_lib + int(split_frac * size_neu))\n",
    "split_index_con = int(size_lib + size_neu + int(split_frac * size_con))\n",
    "\n",
    "\n",
    "val_x, test_x = np.array(var_x[:split_index_lib+1].tolist() + var_x[size_lib:split_index_neu].tolist() + var_x[size_lib+size_neu:split_index_con].tolist()), np.array(var_x[split_index_lib+1:size_lib].tolist() + var_x[split_index_neu:size_lib+size_neu].tolist() + var_x[split_index_con:].tolist()) \n",
    "val_y, test_y = np.array(var_y[:split_index_lib+1].tolist() + var_y[size_lib:split_index_neu].tolist() + var_y[size_lib+size_neu:split_index_con].tolist()), np.array(var_y[split_index_lib+1:size_lib].tolist() + var_y[split_index_neu:size_lib+size_neu].tolist() + var_y[split_index_con:].tolist()) \n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))\n",
    "print(\"label set: \\t\\t{}\".format(train_y.shape), \n",
    "      \"\\nValidation label set: \\t{}\".format(val_y.shape),\n",
    "      \"\\nTest label set: \\t\\t{}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4326"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_size = 256\n",
    "lstm_layers = 2\n",
    "batch_size = 1000\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14013"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = len(vocab_to_int) + 1 # Add 1 for 0 added to vocab\n",
    "# Create the graph object\n",
    "# tf.reset_default_graph()\n",
    "# with tf.name_scope('inputs'):\n",
    "#     inputs_ = tf.placeholder(tf.int32, [None, None], name=\"inputs\")\n",
    "#     labels_ = tf.placeholder(tf.int32, [None, None], name=\"labels\")\n",
    "#     keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 32)            448416    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 501,717\n",
      "Trainable params: 501,717\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.backend import clear_session\n",
    "\n",
    "clear_session()\n",
    "embedding_size=32\n",
    "model=Sequential()\n",
    "model.add(Embedding(n_words, embedding_size, input_length=50))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='tanh'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13563 samples, validate on 1697 samples\n",
      "Epoch 1/50\n",
      "13563/13563 [==============================] - 27s 2ms/step - loss: 1.3896 - acc: 0.3056 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 2/50\n",
      "13563/13563 [==============================] - 28s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 3/50\n",
      "13563/13563 [==============================] - 26s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 4/50\n",
      "13563/13563 [==============================] - 32s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 5/50\n",
      "13563/13563 [==============================] - 25s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 6/50\n",
      "13563/13563 [==============================] - 26s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 7/50\n",
      "13563/13563 [==============================] - 26s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 8/50\n",
      "13563/13563 [==============================] - 28s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 9/50\n",
      "13563/13563 [==============================] - 28s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 10/50\n",
      "13563/13563 [==============================] - 28s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 11/50\n",
      "13563/13563 [==============================] - 28s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 12/50\n",
      "13563/13563 [==============================] - 28s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 13/50\n",
      "13563/13563 [==============================] - 28s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 14/50\n",
      "13563/13563 [==============================] - 28s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 15/50\n",
      "13563/13563 [==============================] - 28s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 16/50\n",
      "13563/13563 [==============================] - 28s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 17/50\n",
      "13563/13563 [==============================] - 28s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 18/50\n",
      "13563/13563 [==============================] - 28s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 19/50\n",
      "13563/13563 [==============================] - 29s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 20/50\n",
      "13563/13563 [==============================] - 29s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 21/50\n",
      "13563/13563 [==============================] - 29s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 22/50\n",
      "13563/13563 [==============================] - 28s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 23/50\n",
      "13563/13563 [==============================] - 28s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 24/50\n",
      "13563/13563 [==============================] - 29s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 25/50\n",
      "13563/13563 [==============================] - 28s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 26/50\n",
      "13563/13563 [==============================] - 29s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 27/50\n",
      "13563/13563 [==============================] - 34s 3ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 28/50\n",
      "13563/13563 [==============================] - 35s 3ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 29/50\n",
      "13563/13563 [==============================] - 32s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 30/50\n",
      "13563/13563 [==============================] - 31s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 31/50\n",
      "13563/13563 [==============================] - 29s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 32/50\n",
      "13563/13563 [==============================] - 30s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 33/50\n",
      "13563/13563 [==============================] - 30s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 34/50\n",
      "13563/13563 [==============================] - 30s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 35/50\n",
      "13563/13563 [==============================] - 28s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 36/50\n",
      "13563/13563 [==============================] - 28s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 37/50\n",
      "13563/13563 [==============================] - 28s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 38/50\n",
      "13563/13563 [==============================] - 28s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 39/50\n",
      "13563/13563 [==============================] - 28s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 40/50\n",
      "13563/13563 [==============================] - 28s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 41/50\n",
      "13563/13563 [==============================] - 28s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 42/50\n",
      "13563/13563 [==============================] - 28s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 43/50\n",
      "13563/13563 [==============================] - 28s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 44/50\n",
      "13563/13563 [==============================] - 30s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 45/50\n",
      "13563/13563 [==============================] - 29s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 46/50\n",
      "13563/13563 [==============================] - 28s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 47/50\n",
      "13563/13563 [==============================] - 30s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 48/50\n",
      "13563/13563 [==============================] - 31s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 49/50\n",
      "13563/13563 [==============================] - 28s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n",
      "Epoch 50/50\n",
      "13563/13563 [==============================] - 29s 2ms/step - loss: 1.6210 - acc: 0.3197 - val_loss: 1.6337 - val_acc: 0.3194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a77280390>"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y, validation_data=(val_x, val_y), batch_size=32, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.31957547169811323\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_x, test_y, verbose=0)\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.engine.sequential.Sequential"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save('models/modelwPhr_tan_b32_e50.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Model So Far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('models/model_tan_b64_e100.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.45958429643793963\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_x, test_y, verbose=0)\n",
    "print('Test accuracy:', scores[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
